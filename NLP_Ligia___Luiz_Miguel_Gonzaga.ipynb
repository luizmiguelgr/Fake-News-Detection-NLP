{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5WruFPZE3d1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Yc_Z_aSrUixc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Centralizando os hiperparâmetros\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 32 # Lote para extração\n",
        "EMBEDDING_DIM = 768"
      ],
      "metadata": {
        "id": "BOtolgeBbzkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arquivos do projeto\n",
        "EMBEDDINGS_FILE = \"bert_embeddings.dat\"\n",
        "CLASSIFIER_MODEL = \"bilstm_classifier.keras\"\n",
        "TOKENIZER_PATH = \"distilbert_tokenizer\"\n",
        "MODEL_CONFIG_FILE = \"model_metadata.pkl\""
      ],
      "metadata": {
        "id": "g0n4LT8KHPSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "p_QLcArZVLme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/NLP Ligia | Luiz Miguel Gonzaga | FAKE NEWS Detection\"\n",
        "\n",
        "train_path = f\"{BASE_PATH}/train.csv\"\n",
        "test_path = f\"{BASE_PATH}/test.csv\"\n",
        "\n",
        "df = pd.read_csv(train_path)"
      ],
      "metadata": {
        "id": "_y-_21dqFZlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EDA**"
      ],
      "metadata": {
        "id": "Ibn7OHBVHqzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "ct = pd.crosstab(df[\"subject\"], df[\"label\"], normalize=\"index\")\n",
        "sns.heatmap(ct, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlação Subject vs Fake\")\n",
        "plt.show()\n",
        "\n",
        "df_model = df.drop(columns=[\"subject\", \"date\", \"id\"])\n",
        "# Análise da correlação entre o veículo de notícias e a quantidade de casos verdadeiros e falsos"
      ],
      "metadata": {
        "id": "NTA97Zp1Hk46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df_model, x=\"label\")\n",
        "plt.title(\"Distribuição das Classes\")\n",
        "plt.show()\n",
        "\n",
        "print(df_model[\"label\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "yXGrTFWAU7XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model[\"title_len\"] = df_model[\"title\"].str.split().apply(len)\n",
        "df_model[\"text_len\"] = df_model[\"text\"].str.split().apply(len)\n",
        "\n",
        "print(df_model[[\"title_len\",\"text_len\"]].describe(percentiles=[0.5,0.75,0.9,0.95,0.99]))\n",
        "# Dados Estatísticos, do tamanho do título e texto"
      ],
      "metadata": {
        "id": "ZH4UWOEVU_rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HlPt98kRVWsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Junta todos os textos, junta as palavras mais ditas, a frequência de cada\n",
        "texto_total = \" \".join(df[\"text\"].dropna())\n",
        "palavras = texto_total.split()\n",
        "from collections import Counter\n",
        "contagem = Counter(palavras)\n",
        "print(contagem.most_common(40))"
      ],
      "metadata": {
        "id": "rGOhOmfIVk9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pré-processamento**"
      ],
      "metadata": {
        "id": "2TcU0LJZV-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\"subject\", \"date\"], errors=\"ignore\")\n",
        "\n",
        "def clean_text_light(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = re.sub(r\"\\(?reuters\\)?\\s*-?\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_light\"] = (df[\"title\"] + \" \" + df[\"text\"]).apply(clean_text_light)\n",
        "\n",
        "X_text = df[\"text_light\"].values\n",
        "y = df[\"label\"].values\n",
        "N_SAMPLES = len(X_text)\n",
        "# Remove Link, veículo de notícias, múltiplos espaços"
      ],
      "metadata": {
        "id": "K1e8v6puV9Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extranindo Embbedings: BERT e MEMMAP**"
      ],
      "metadata": {
        "id": "G2ESzT-fX928"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega tokenizer e modelo DistilBERT pré-treinado\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "bert_model.eval()  # Modo avaliação (desativa dropout)"
      ],
      "metadata": {
        "id": "4SjlGfFVX4wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se já existe arquivo antigo de embeddings, remove\n",
        "if os.path.exists(EMBEDDINGS_FILE):\n",
        "    os.remove(EMBEDDINGS_FILE)"
      ],
      "metadata": {
        "id": "3ruSDmGmYFSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aloca espaço no disco\n",
        "X_memmap = np.memmap(\n",
        "    EMBEDDINGS_FILE,\n",
        "    dtype=\"float32\",\n",
        "    mode=\"w+\",\n",
        "    shape=(N_SAMPLES, MAX_LEN, EMBEDDING_DIM)\n",
        ")"
      ],
      "metadata": {
        "id": "Y6a4ehTTYNSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trunca texto: pega 382 do começo e 128 do final se passar de 512 tokens\n",
        "def smart_truncate(text, tokenizer, max_len=512):\n",
        "    limit = max_len - 2\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(tokens) <= limit:\n",
        "        return tokenizer.encode(text, max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "    head = tokens[:382]\n",
        "    tail = tokens[-128:]\n",
        "    return [tokenizer.cls_token_id] + head + tail + [tokenizer.sep_token_id]"
      ],
      "metadata": {
        "id": "PtOm3OJxYRA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(0, N_SAMPLES, BATCH_SIZE)):\n",
        "    batch_texts = X_text[i : i + BATCH_SIZE]\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in batch_texts:\n",
        "        ids = smart_truncate(text, tokenizer, MAX_LEN)\n",
        "        # Pad manual\n",
        "        if len(ids) < MAX_LEN:\n",
        "            ids = ids + [tokenizer.pad_token_id] * (MAX_LEN - len(ids))\n",
        "\n",
        "        mask = [1 if token != tokenizer.pad_token_id else 0 for token in ids]\n",
        "        input_ids.append(ids)\n",
        "        attention_masks.append(mask)\n",
        "\n",
        "    input_ids_pt = torch.tensor(input_ids).to(device)\n",
        "    attention_masks_pt = torch.tensor(attention_masks).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(input_ids_pt, attention_mask=attention_masks_pt)\n",
        "        batch_embeddings = outputs.last_hidden_state\n",
        "\n",
        "        # Aplica a máscara para zerar padding\n",
        "        mask_expanded = attention_masks_pt.unsqueeze(-1).expand(batch_embeddings.size()).float()\n",
        "        batch_embeddings = batch_embeddings * mask_expanded\n",
        "\n",
        "        # Escreve no Memmap (Disco)\n",
        "        # O .cpu().numpy() tira da GPU e joga pro disco via memmap\n",
        "        X_memmap[i : i + len(batch_texts)] = batch_embeddings.cpu().numpy()\n",
        "\n",
        "        # Força gravação no disco para liberar RAM\n",
        "        X_memmap.flush()\n",
        "\n",
        "# Limpa GPU\n",
        "del bert_model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mu9-ryYcZHcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gerador e Split**"
      ],
      "metadata": {
        "id": "n8I-Ru8UeN_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemmapGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, memmap_data, indices, labels, batch_size=16):\n",
        "        self.memmap_data = memmap_data\n",
        "        self.indices = indices\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X_batch = self.memmap_data[batch_indices]\n",
        "        y_batch = self.labels[batch_indices]\n",
        "        return X_batch, y_batch\n",
        "\n",
        "\n",
        "indices = np.arange(N_SAMPLES)"
      ],
      "metadata": {
        "id": "KQZlhQ5NePvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, val_idx, y_train_split, y_val_split = train_test_split(\n",
        "    indices, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "weights = class_weight.compute_class_weight(\n",
        "    \"balanced\",\n",
        "    classes=np.unique(y_train_split),\n",
        "    y=y_train_split\n",
        ")\n",
        "\n",
        "class_weights = dict(zip(np.unique(y_train_split), weights))\n",
        "\n",
        "X_memmap_read = np.memmap(\n",
        "    EMBEDDINGS_FILE,\n",
        "    dtype=\"float32\",\n",
        "    mode=\"r\",\n",
        "    shape=(N_SAMPLES, MAX_LEN, EMBEDDING_DIM)\n",
        ")\n",
        "\n",
        "train_gen = MemmapGenerator(X_memmap_read, train_idx, y, batch_size=16)\n",
        "val_gen = MemmapGenerator(X_memmap_read, val_idx, y, batch_size=16)"
      ],
      "metadata": {
        "id": "GaEhAaDxeWeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Treinamento**"
      ],
      "metadata": {
        "id": "XO0bOhTnec1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_classifier():\n",
        "\n",
        "    # Modelo recebe embeddings do BERT (MAX_LEN x 768) e usa BiLSTM para capturar contexto\n",
        "    inputs = tf.keras.Input(shape=(MAX_LEN, EMBEDDING_DIM))\n",
        "    x = tf.keras.layers.Masking(mask_value=0.0)(inputs)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(x)\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_lstm_classifier()\n",
        "\n",
        "# Callbacks para salvar melhor modelo e parar treino se não melhorar\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"best_lstm.keras\",\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Treina o classificador usando embeddings pré-gerados\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "EiMbdxtjeebv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Avaliação e Limite**"
      ],
      "metadata": {
        "id": "tdkpsCMGtvTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = model.predict(val_gen)\n",
        "y_true_val = y[val_idx]\n",
        "\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "\n",
        "f1_scores = [\n",
        "    f1_score(y_true_val, (y_pred_prob > t).astype(int))\n",
        "    for t in thresholds\n",
        "]\n",
        "\n",
        "best_thresh = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "print(\"Melhor Threshold:\", best_thresh)\n",
        "print(\"Melhor F1:\", max(f1_scores))\n",
        "print(classification_report(\n",
        "    y_true_val,\n",
        "    (y_pred_prob > best_thresh).astype(int),\n",
        "    digits=4\n",
        "))"
      ],
      "metadata": {
        "id": "0dpgYfHzt0WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Salvar**"
      ],
      "metadata": {
        "id": "Rj-a9OMPt4Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(MODEL_FILE)\n",
        "tokenizer.save_pretrained(TOKENIZER_FOLDER)\n",
        "\n",
        "configuracoes = {\n",
        "    \"best_threshold\": best_thresh,\n",
        "    \"max_len\": MAX_LEN\n",
        "}\n",
        "\n",
        "with open(CONFIG_FILE, \"wb\") as f:\n",
        "    pickle.dump(configuracoes, f)"
      ],
      "metadata": {
        "id": "Qb1pnqgGt55x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretação das Métricas**"
      ],
      "metadata": {
        "id": "nnnpSTFUt9KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef, precision_recall_curve\n",
        "\n",
        "y_pred_final = (y_pred_prob > best_thresh).astype(int)\n",
        "mcc = matthews_corrcoef(y_true_val, y_pred_final)\n",
        "print(\"MCC:\", mcc)\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true_val, y_pred_prob)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n",
        "\n",
        "def analyze_sensitivity(sample_index, remove_tokens=50):\n",
        "    original_embedding = X_memmap_read[sample_index:sample_index+1]\n",
        "    original_prob = model.predict(original_embedding)[0][0]\n",
        "\n",
        "    perturbed_embedding = original_embedding.copy()\n",
        "    perturbed_embedding[:, :remove_tokens, :] = 0\n",
        "\n",
        "    new_prob = model.predict(perturbed_embedding)[0][0]\n",
        "\n",
        "    print(\"Prob original:\", float(original_prob))\n",
        "    print(\"Prob perturbado:\", float(new_prob))\n",
        "    print(\"Variação:\", abs(float(original_prob - new_prob)))"
      ],
      "metadata": {
        "id": "lDO_NjlAt_DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline para novo conjunto**"
      ],
      "metadata": {
        "id": "YQiIz-rRuLu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(CONFIG_FILE, \"rb\") as f:\n",
        "    configuracoes = pickle.load(f)\n",
        "\n",
        "best_thresh = configuracoes[\"best_threshold\"]\n",
        "MAX_LEN = configuracoes[\"max_len\"]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(TOKENIZER_FOLDER)\n",
        "model = tf.keras.models.load_model(MODEL_FILE)\n",
        "\n",
        "bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "df_test[\"title\"] = df_test[\"title\"].fillna(\"\")\n",
        "df_test[\"text\"] = df_test[\"text\"].fillna(\"\")\n",
        "\n",
        "df_test[\"text_light\"] = (df_test[\"title\"] + \" \" + df_test[\"text\"]).apply(clean_text_light)\n",
        "X_test_text = df_test[\"text_light\"].values\n",
        "\n",
        "todas_probabilidades = []\n",
        "\n",
        "for i in tqdm(range(0, len(X_test_text), BATCH_SIZE)):\n",
        "    batch_texts = X_test_text[i:i+BATCH_SIZE]\n",
        "\n",
        "    input_ids, attention_masks = [], []\n",
        "\n",
        "    for text in batch_texts:\n",
        "        ids = smart_truncate(text, tokenizer, MAX_LEN)\n",
        "        if len(ids) < MAX_LEN:\n",
        "            ids += [tokenizer.pad_token_id] * (MAX_LEN - len(ids))\n",
        "        mask = [1 if token != tokenizer.pad_token_id else 0 for token in ids]\n",
        "        input_ids.append(ids)\n",
        "        attention_masks.append(mask)\n",
        "\n",
        "    input_ids_pt = torch.tensor(input_ids).to(device)\n",
        "    attention_masks_pt = torch.tensor(attention_masks).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(input_ids_pt, attention_mask=attention_masks_pt)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "        mask_expanded = attention_masks_pt.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "        embeddings = embeddings * mask_expanded\n",
        "\n",
        "    preds = model.predict(embeddings.cpu().numpy(), verbose=0)\n",
        "    todas_probabilidades.extend(preds.flatten())\n",
        "\n",
        "y_pred_test = (np.array(todas_probabilidades) > best_thresh).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": df_test[\"id\"],\n",
        "    \"label\": y_pred_test\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "WEBCM3muuNsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}